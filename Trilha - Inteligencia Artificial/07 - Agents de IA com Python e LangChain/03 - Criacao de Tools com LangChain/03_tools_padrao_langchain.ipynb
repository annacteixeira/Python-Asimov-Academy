{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools padrão do LangChain\n",
    "\n",
    "A biblioteca LangChain já possui diversas ferramentas padrão construídas que podemos utilizar. Elas podem ser verificadas neste link: https://python.langchain.com/v0.1/docs/integrations/tools/.\n",
    "Vamos mostrar como utilizar algumas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ArXiv\n",
    "\n",
    "Esta ferramenta utiliza a biblioteca do arXiv para retornar resumos de artigos científicos de um determinado tema solicitado.\n",
    "\n",
    "https://python.langchain.com/v0.1/docs/integrations/tools/arxiv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos três formas em geral para chamar uma tool, da mais baixo a mais alto nível:\n",
    "- Criando a tool utilziando o Wrapper: no LangChain foram criados diferentes wrappers para apis e bibliotecas externas que facilitam a utilização das mesmas e permitem uma rápida criação de uma nova tool. Ela é mais customizável, pois podemos modificar as descrições e argumentos da tool às nossa necessidades, além de podermos modificar facilmente os parâmetros do wrapper\n",
    "- Instanciando a tool já criada: em geral, uma tool pronta já está criada dentro da biblioteca e podemos acessá-la diretamente\n",
    "- Utilizando o load_tools: o LangChain possui uma ferramnta especial chamada load_tools que permite o carregamento de diversas ferramentas ao mesmo tempo\n",
    "\n",
    "Vamos explorar os três métodos agora:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando tool manualmente através do Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities.arxiv import ArxivAPIWrapper\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.tools import StructuredTool\n",
    "\n",
    "class ArxivArgs(BaseModel):\n",
    "    query:str = Field(description='Qyuery de busca no ArXiv')\n",
    "\n",
    "tool_arxiv = StructuredTool.from_function(\n",
    "    func=ArxivAPIWrapper(top_k_results=2).run,\n",
    "    args_schema=ArxivArgs,\n",
    "    name='arxiv',\n",
    "    description = (\n",
    "        \"Uma ferramenta em torno do Arxiv.org. \"\n",
    "        \"Útil para quando você precisa responder a perguntas sobre Física, Matemática, \"\n",
    "        \"Ciência da Computação, Biologia Quantitativa, Finanças Quantitativas, Estatística, \"\n",
    "        \"Engenharia Elétrica e Economia utilizando artigos científicos do arxiv.org. \"\n",
    "        \"A entrada deve ser uma consulta de pesquisa em inglês.\"\n",
    "    ),\n",
    "    return_direct=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descrição: arxiv(query: str) -> str - Uma ferramenta em torno do Arxiv.org. Útil para quando você precisa responder a perguntas sobre Física, Matemática, Ciência da Computação, Biologia Quantitativa, Finanças Quantitativas, Estatística, Engenharia Elétrica e Economia utilizando artigos científicos do arxiv.org. A entrada deve ser uma consulta de pesquisa em inglês.\n",
      "Args: {'query': {'title': 'Query', 'description': 'Qyuery de busca no ArXiv', 'type': 'string'}}\n"
     ]
    }
   ],
   "source": [
    "print('Descrição:', tool_arxiv.description)\n",
    "print('Args:', tool_arxiv.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Published: 2024-12-23\\nTitle: Trustworthy and Efficient LLMs Meet Databases\\nAuthors: Kyoungmin Kim, Anastasia Ailamaki\\nSummary: In the rapidly evolving AI era with large language models (LLMs) at the core,\\nmaking LLMs more trustworthy and efficient, especially in output generation\\n(inference), has gained significant attention. This is to reduce plausible but\\nfaulty LLM outputs (a.k.a hallucinations) and meet the highly increased\\ninference demands. This tutorial explores such efforts and makes them\\ntransparent to the database community. Understanding these efforts is essential\\nin harnessing LLMs in database tasks and adapting database techniques to LLMs.\\nFurthermore, we delve into the synergy between LLMs and databases, highlighting\\nnew opportunities and challenges in their intersection. This tutorial aims to\\nshare with database researchers and practitioners essential concepts and\\nstrategies around LLMs, reduce the unfamiliarity of LLMs, and inspire joining\\nin the intersection between LLMs and databases.\\n\\nPublished: 2024-06-13\\nTitle: Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications\\nAuthors: Irene Weber\\nSummary: Large Language Models (LLMs) have become widely adopted recently. Research\\nexplores their use both as autonomous agents and as tools for software\\nengineering. LLM-integrated applications, on the other hand, are software\\nsystems that leverage an LLM to perform tasks that would otherwise be\\nimpossible or require significant coding effort. While LLM-integrated\\napplication engineering is emerging as new discipline, its terminology,\\nconcepts and methods need to be established. This study provides a taxonomy for\\nLLM-integrated applications, offering a framework for analyzing and describing\\nthese systems. It also demonstrates various ways to utilize LLMs in\\napplications, as well as options for implementing such integrations.\\n  Following established methods, we analyze a sample of recent LLM-integrated\\napplications to identify relevant dimensions. We evaluate the taxonomy by\\napplying it to additional cases. This review shows that applications integrate\\nLLMs in numerous ways for various purposes. Frequently, they comprise multiple\\nLLM integrations, which we term ``LLM components''. To gain a clear\\nunderstanding of an application's architecture, we examine each LLM component\\nseparately. We identify thirteen dimensions along which to characterize an LLM\\ncomponent, including the LLM skills leveraged, the format of the output, and\\nmore. LLM-integrated applications are described as combinations of their LLM\\ncomponents. We suggest a concise representation using feature vectors for\\nvisualization.\\n  The taxonomy is effective for describing LLM-integrated applications. It can\\ncontribute to theory building in the nascent field of LLM-integrated\\napplication engineering and aid in developing such systems. Researchers and\\npractitioners explore numerous creative ways to leverage LLMs in applications.\\nThough challenges persist, integrating LLMs may revolutionize the way software\\nsystems are built.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_arxiv.run({'query': 'llm'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instanciando tool já criada no LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
    "\n",
    "tool_arxiv = ArxivQueryRun(api_wrapper=ArxivAPIWrapper(top_k_results=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descrição: A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.\n",
      "Args: {'query': {'title': 'Query', 'description': 'search query to look up', 'type': 'string'}}\n"
     ]
    }
   ],
   "source": [
    "print('Descrição:', tool_arxiv.description)\n",
    "print('Args:', tool_arxiv.args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando tool através do load_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ArxivQueryRun()]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = load_tools(['arxiv'])\n",
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ArxivQueryRun()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_arxiv = tools[0]\n",
    "tool_arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descrição: A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.\n",
      "Args: {'query': {'title': 'Query', 'description': 'search query to look up', 'type': 'string'}}\n"
     ]
    }
   ],
   "source": [
    "print('Descrição:', tool_arxiv.description)\n",
    "print('Args:', tool_arxiv.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Published: 2024-12-23\\nTitle: Trustworthy and Efficient LLMs Meet Databases\\nAuthors: Kyoungmin Kim, Anastasia Ailamaki\\nSummary: In the rapidly evolving AI era with large language models (LLMs) at the core,\\nmaking LLMs more trustworthy and efficient, especially in output generation\\n(inference), has gained significant attention. This is to reduce plausible but\\nfaulty LLM outputs (a.k.a hallucinations) and meet the highly increased\\ninference demands. This tutorial explores such efforts and makes them\\ntransparent to the database community. Understanding these efforts is essential\\nin harnessing LLMs in database tasks and adapting database techniques to LLMs.\\nFurthermore, we delve into the synergy between LLMs and databases, highlighting\\nnew opportunities and challenges in their intersection. This tutorial aims to\\nshare with database researchers and practitioners essential concepts and\\nstrategies around LLMs, reduce the unfamiliarity of LLMs, and inspire joining\\nin the intersection between LLMs and databases.\\n\\nPublished: 2024-06-13\\nTitle: Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications\\nAuthors: Irene Weber\\nSummary: Large Language Models (LLMs) have become widely adopted recently. Research\\nexplores their use both as autonomous agents and as tools for software\\nengineering. LLM-integrated applications, on the other hand, are software\\nsystems that leverage an LLM to perform tasks that would otherwise be\\nimpossible or require significant coding effort. While LLM-integrated\\napplication engineering is emerging as new discipline, its terminology,\\nconcepts and methods need to be established. This study provides a taxonomy for\\nLLM-integrated applications, offering a framework for analyzing and describing\\nthese systems. It also demonstrates various ways to utilize LLMs in\\napplications, as well as options for implementing such integrations.\\n  Following established methods, we analyze a sample of recent LLM-integrated\\napplications to identify relevant dimensions. We evaluate the taxonomy by\\napplying it to additional cases. This review shows that applications integrate\\nLLMs in numerous ways for various purposes. Frequently, they comprise multiple\\nLLM integrations, which we term ``LLM components''. To gain a clear\\nunderstanding of an application's architecture, we examine each LLM component\\nseparately. We identify thirteen dimensions along which to characterize an LLM\\ncomponent, including the LLM skills leveraged, the format of the output, and\\nmore. LLM-integrated applications are described as combinations of their LLM\\ncomponents. We suggest a concise representation using feature vectors for\\nvisualization.\\n  The taxonomy is effective for describing LLM-integrated applications. It can\\ncontribute to theory building in the nascent field of LLM-integrated\\napplication engineering and aid in developing such systems. Researchers and\\npractitioners explore numerous creative ways to leverage LLMs in applications.\\nThough challenges persist, integrating LLMs may revolutionize the way software\\nsystems are built.\\n\\nPublished: 2024-05-30\\nTitle: Parrot: Efficient Serving of LLM-based Applications with Semantic Variable\\nAuthors: Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu\\nSummary: The rise of large language models (LLMs) has enabled LLM-based applications\\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\\nstrength of LLM and conventional software. Diverse LLM applications from\\ndifferent tenants could design complex workflows using multiple LLM requests to\\naccomplish one task. However, they have to use the over-simplified\\nrequest-level API provided by today's public LLM services, losing essential\\napplication-level information. Public LLM services have to blindly optimize\\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\\napplications.\\n  This paper introduces Parrot, an LLM service system that focuses on the\\nend-to-end experience of \""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_arxiv.run({'query': 'llm'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python REPL\n",
    "\n",
    "https://python.langchain.com/v0.1/docs/integrations/tools/python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descrição: A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\n",
      "Args: {'query': {'title': 'Query', 'type': 'string'}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "\n",
    "tool_repl = PythonREPLTool()\n",
    "print('Descrição:', tool_repl.description)\n",
    "print('Args:', tool_repl.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'oi\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_repl.run({'query': 'print(\"oi\")'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StackOverflow\n",
    "\n",
    "https://python.langchain.com/v0.1/docs/integrations/tools/stackexchange/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descrição: A wrapper around StackExchange. Useful for when you need to answer specific programming questionscode excerpts, code examples and solutionsInput should be a fully formed question.\n",
      "Args: {'query': {'title': 'Query', 'type': 'string'}}\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import load_tools\n",
    "\n",
    "tools = load_tools(['stackexchange'])\n",
    "tool_stack = tools[0]\n",
    "print('Descrição:', tool_stack.description)\n",
    "print('Args:', tool_stack.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question: Problem Generating SQL Script with SQL LangChain Agent\\nI&#39;m trying to implement an SQL <span class=\"highlight\">LangChain</span> <span class=\"highlight\">agent</span> to process an existing SQLite database. &hellip; \\n\\nQuestion: Tool Methods in Python Class Missing self Argument When Used in Custom Agents\\nI am working on a custom implementation of AI <span class=\"highlight\">agents</span> using Python, where the <span class=\"highlight\">agents</span> rely on <span class=\"highlight\">langchain</span> tools (methods) for interacting with a PostgreSQL database. &hellip; These tools are later used by CrewAI <span class=\"highlight\">agents</span> to perform various tasks. &hellip; \\n\\nQuestion: How do I pipe an Output Parser to Langchain&#39;s agent?\\nThis is possible with non-<span class=\"highlight\">agent</span>, but I can&#39;t find any documents on piping Output Parser to <span class=\"highlight\">agent</span>.\\nimport { TavilySearchResults } from &quot;@<span class=\"highlight\">langchain</span>/community/tools/tavily_search&quot;;\\nimport { ChatOpenAI } from &hellip; &quot;@<span class=\"highlight\">langchain</span>/openai&quot;;\\nimport { ChatPromptTemplate, MessagesPlaceholder } from &quot;@<span class=\"highlight\">langchain</span>/core/prompts&quot;;\\nimport { z } from &quot;zod&quot;;\\nimport { createOpenAIToolsAgent, AgentExecutor } from &quot;<span class=\"highlight\">langchain</span>/<span class=\"highlight\">agents</span> &hellip; '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_stack.run({'query': 'LangChain Agents'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File System\n",
    "\n",
    "https://python.langchain.com/v0.1/docs/integrations/tools/filesystem/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: copy_file\n",
      "Descrição: Create a copy of a file in a specified location\n",
      "Args: {'source_path': {'title': 'Source Path', 'description': 'Path of the file to copy', 'type': 'string'}, 'destination_path': {'title': 'Destination Path', 'description': 'Path to save the copied file', 'type': 'string'}}\n",
      "\n",
      "Name: file_delete\n",
      "Descrição: Delete a file\n",
      "Args: {'file_path': {'title': 'File Path', 'description': 'Path of the file to delete', 'type': 'string'}}\n",
      "\n",
      "Name: file_search\n",
      "Descrição: Recursively search for files in a subdirectory that match the regex pattern\n",
      "Args: {'dir_path': {'title': 'Dir Path', 'description': 'Subdirectory to search in.', 'default': '.', 'type': 'string'}, 'pattern': {'title': 'Pattern', 'description': 'Unix shell regex, where * matches everything.', 'type': 'string'}}\n",
      "\n",
      "Name: move_file\n",
      "Descrição: Move or rename a file from one location to another\n",
      "Args: {'source_path': {'title': 'Source Path', 'description': 'Path of the file to move', 'type': 'string'}, 'destination_path': {'title': 'Destination Path', 'description': 'New path for the moved file', 'type': 'string'}}\n",
      "\n",
      "Name: read_file\n",
      "Descrição: Read file from disk\n",
      "Args: {'file_path': {'title': 'File Path', 'description': 'name of file', 'type': 'string'}}\n",
      "\n",
      "Name: write_file\n",
      "Descrição: Write file to disk\n",
      "Args: {'file_path': {'title': 'File Path', 'description': 'name of file', 'type': 'string'}, 'text': {'title': 'Text', 'description': 'text to write to file', 'type': 'string'}, 'append': {'title': 'Append', 'description': 'Whether to append to an existing file.', 'default': False, 'type': 'boolean'}}\n",
      "\n",
      "Name: list_directory\n",
      "Descrição: List files and directories in a specified folder\n",
      "Args: {'dir_path': {'title': 'Dir Path', 'description': 'Subdirectory to list.', 'default': '.', 'type': 'string'}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.agent_toolkits.file_management.toolkit import FileManagementToolkit\n",
    "\n",
    "tool_kit = FileManagementToolkit(\n",
    "    root_dir='arquivos'\n",
    ")\n",
    "tools = tool_kit.get_tools()\n",
    "for tool in tools:\n",
    "    print('Name:', tool.name)\n",
    "    print('Descrição:', tool.description)\n",
    "    print('Args:', tool.args) \n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dando um exemplo de aplicação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.agent_toolkits.file_management.toolkit import FileManagementToolkit\n",
    "\n",
    "tool_kit = FileManagementToolkit(\n",
    "    root_dir='files',\n",
    "    selected_tools=['write_file', 'read_file', 'file_search','list_directory']\n",
    ")\n",
    "tools = tool_kit.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "tools_json = [convert_to_openai_function(tool) for tool in tools]\n",
    "tool_run = {tool.name: tool for tool in tools}\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'Você é um assistente amigável chamado Isaac capaz de gerenciar arquivos'),\n",
    "    ('user', '{input}')\n",
    "])\n",
    "chat = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent import AgentFinish\n",
    "\n",
    "def roteamento(resultado):\n",
    "    if isinstance(resultado, AgentFinish):\n",
    "        return resultado.return_values['output']\n",
    "    else:\n",
    "        return tool_run[resultado.tool].run(resultado.tool_input)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "\n",
    "\n",
    "chain = prompt | chat.bind(functions=tools_json) | OpenAIFunctionsAgentOutputParser() | roteamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Olá! Eu sou um assistente chamado Isaac e sou capaz de gerenciar arquivos. Posso escrever texto em arquivos, ler arquivos do disco, procurar arquivos que correspondam a um padrão específico e listar arquivos e diretórios em uma pasta especificada. Se precisar de ajuda com alguma dessas tarefas, fique à vontade para me pedir!'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'input': 'O que você é capaz de fazer?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Explorando o Universo das IAs com Hugging Face.pdf'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'input': 'Quais arquivos pdf você tem na pasta?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No files found for pattern *.txt in directory .'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'input': 'Quais arquivos txt você tem na pasta?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'File written successfully to notas.txt.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'input': 'Crie um arquivo txt chamado notas com o seguinte conteúdo \"Isso foi feito por uma LLM\"'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Isso foi feito por uma LLM'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'input': 'Leia o arquivo notas.txt'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
