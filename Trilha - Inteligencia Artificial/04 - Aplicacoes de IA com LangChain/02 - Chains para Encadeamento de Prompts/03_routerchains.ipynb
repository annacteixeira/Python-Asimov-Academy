{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RouterChains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Router Chains são cadeias mais avançadas que permitem roteamento de prompts com base na entrada do usuário. Elas são úteis quando você precisa lidar com diferentes tipos de perguntas ou problemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annaf\\AppData\\Local\\Temp\\ipykernel_10496\\1267886439.py:6: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  llm = OpenAI(model='gpt-3.5-turbo-instruct')\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model='gpt-3.5-turbo-instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fis_template = ChatPromptTemplate.from_template(\"\"\"Você é um professor de física muito inteligente.\n",
    "Você é ótimo em responder perguntas sobre física de forma concisa\n",
    "e fácil de entender.\n",
    "Quando você não sabe a resposta para uma pergunta, você admite\n",
    "que não sabe.\n",
    "\n",
    "Aqui está uma pergunta: {input}\"\"\")\n",
    "\n",
    "mat_template = ChatPromptTemplate.from_template(\"\"\"Você é um matemático muito bom.\n",
    "Você é ótimo em responder perguntas de matemática.\n",
    "Você é tão bom porque consegue decompor\n",
    "problemas difíceis em suas partes componentes, \n",
    "responder às partes componentes e depois juntá-las\n",
    "para responder à pergunta mais ampla.\n",
    "\n",
    "Aqui está uma pergunta: {input}\"\"\")\n",
    "\n",
    "hist_template = ChatPromptTemplate.from_template(\"\"\"Você é um historiador muito bom.\n",
    "Você tem um excelente conhecimento e compreensão de pessoas,\n",
    "eventos e contextos de uma variedade de períodos históricos.\n",
    "Você tem a capacidade de pensar, refletir, debater, discutir e\n",
    "avaliar o passado. Você tem respeito pela evidência histórica\n",
    "e a capacidade de usá-la para apoiar suas explicações\n",
    "e julgamentos.\n",
    "\n",
    "Aqui está uma pergunta: {input}\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        'name': 'Física',\n",
    "        'description': 'Ideal para responder perguntas sobre Física',\n",
    "        'prompt_template': fis_template,\n",
    "    },\n",
    "    {\n",
    "        'name': 'Matemática',\t\n",
    "        'description': 'Ideal para responder perguntas sobre Matemática',\n",
    "        'prompt_template': mat_template,\n",
    "    },\n",
    "    {\n",
    "        'name': 'História',\t\n",
    "        'description': 'Ideal para responder perguntas sobre História',\n",
    "        'prompt_template': hist_template,\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(model='gpt-3.5-turbo-0125')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annaf\\AppData\\Local\\Temp\\ipykernel_10496\\1188194747.py:4: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=chat, prompt=info['prompt_template'], verbose=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Física': LLMChain(verbose=True, prompt=ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='Você é um professor de física muito inteligente.\\nVocê é ótimo em responder perguntas sobre física de forma concisa\\ne fácil de entender.\\nQuando você não sabe a resposta para uma pergunta, você admite\\nque não sabe.\\n\\nAqui está uma pergunta: {input}'), additional_kwargs={})]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001BF582DEF60>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001BF582DFE60>, root_client=<openai.OpenAI object at 0x000001BF44B30740>, root_async_client=<openai.AsyncOpenAI object at 0x000001BF582DEF00>, model_name='gpt-3.5-turbo-0125', model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}),\n",
       " 'Matemática': LLMChain(verbose=True, prompt=ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='Você é um matemático muito bom.\\nVocê é ótimo em responder perguntas de matemática.\\nVocê é tão bom porque consegue decompor\\nproblemas difíceis em suas partes componentes, \\nresponder às partes componentes e depois juntá-las\\npara responder à pergunta mais ampla.\\n\\nAqui está uma pergunta: {input}'), additional_kwargs={})]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001BF582DEF60>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001BF582DFE60>, root_client=<openai.OpenAI object at 0x000001BF44B30740>, root_async_client=<openai.AsyncOpenAI object at 0x000001BF582DEF00>, model_name='gpt-3.5-turbo-0125', model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}),\n",
       " 'História': LLMChain(verbose=True, prompt=ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='Você é um historiador muito bom.\\nVocê tem um excelente conhecimento e compreensão de pessoas,\\neventos e contextos de uma variedade de períodos históricos.\\nVocê tem a capacidade de pensar, refletir, debater, discutir e\\navaliar o passado. Você tem respeito pela evidência histórica\\ne a capacidade de usá-la para apoiar suas explicações\\ne julgamentos.\\n\\nAqui está uma pergunta: {input}'), additional_kwargs={})]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001BF582DEF60>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001BF582DFE60>, root_client=<openai.OpenAI object at 0x000001BF44B30740>, root_async_client=<openai.AsyncOpenAI object at 0x000001BF582DEF00>, model_name='gpt-3.5-turbo-0125', model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={})}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chains_destino = {}\n",
    "\n",
    "for info in prompt_infos:\n",
    "    chain = LLMChain(llm=chat, prompt=info['prompt_template'], verbose=True)\n",
    "    chains_destino[info['name']] = chain\n",
    "    \n",
    "chains_destino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Física: Ideal para responder perguntas sobre Física\n",
      "Matemática: Ideal para responder perguntas sobre Matemática\n",
      "História: Ideal para responder perguntas sobre História\n"
     ]
    }
   ],
   "source": [
    "destinos = [f'{p[\"name\"]}: {p[\"description\"]}' for p in prompt_infos]\n",
    "destinos_str = '\\n'.join(destinos)\n",
    "print(destinos_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.\n",
      "\n",
      "<< FORMATTING >>\n",
      "Return a markdown code snippet with a JSON object formatted to look like:\n",
      "```json\n",
      "{{\n",
      "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
      "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
      "}}\n",
      "```\n",
      "\n",
      "REMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is not well suited for any of the candidate prompts.\n",
      "REMEMBER: \"next_inputs\" can just be the original input if you don't think any modifications are needed.\n",
      "\n",
      "<< CANDIDATE PROMPTS >>\n",
      "Física: Ideal para responder perguntas sobre Física\n",
      "Matemática: Ideal para responder perguntas sobre Matemática\n",
      "História: Ideal para responder perguntas sobre História\n",
      "\n",
      "<< INPUT >>\n",
      "{input}\n",
      "\n",
      "<< OUTPUT (must include ```json at the start of the response) >>\n",
      "<< OUTPUT (must end with ```) >>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations = destinos_str\n",
    ")\n",
    "\n",
    "print(router_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_template = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=['input'],\n",
    "    output_parser=RouterOutputParser()\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(chat, router_template, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annaf\\AppData\\Local\\Temp\\ipykernel_10496\\2250053747.py:4: LangChainDeprecationWarning: Please see migration guide here for recommended implementation: https://python.langchain.com/docs/versions/migrating_chains/multi_prompt_chain/\n",
      "  chain = MultiPromptChain(\n"
     ]
    }
   ],
   "source": [
    "default_prompt = ChatPromptTemplate.from_template('{input}')\n",
    "default_chain = LLMChain(llm=chat, prompt=default_prompt, verbose=True)\n",
    "\n",
    "chain = MultiPromptChain(\n",
    "    router_chain=router_chain,\n",
    "    destination_chains=chains_destino,\n",
    "    default_chain=default_chain,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMRouterChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Física: {'input': 'O que é um buraco negro?'}\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: Você é um professor de física muito inteligente.\n",
      "Você é ótimo em responder perguntas sobre física de forma concisa\n",
      "e fácil de entender.\n",
      "Quando você não sabe a resposta para uma pergunta, você admite\n",
      "que não sabe.\n",
      "\n",
      "Aqui está uma pergunta: O que é um buraco negro?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'O que é um buraco negro?',\n",
       " 'text': 'Um buraco negro é uma região do espaço-tempo onde a gravidade é tão intensa que nada, nem mesmo a luz, consegue escapar de seu campo gravitacional. Ele é formado a partir do colapso de uma estrela massiva no final de sua vida. A massa é concentrada em um ponto de densidade infinita chamado singularidade, cercado por uma região de onde nada pode escapar, chamada de horizonte de eventos. Os buracos negros são objetos extremamente densos e exibem propriedades únicas devido à intensidade de sua gravidade.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'input': 'O que é um buraco negro?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMRouterChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Matemática: {'input': 'O que é uma equação quadrática?'}\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: Você é um matemático muito bom.\n",
      "Você é ótimo em responder perguntas de matemática.\n",
      "Você é tão bom porque consegue decompor\n",
      "problemas difíceis em suas partes componentes, \n",
      "responder às partes componentes e depois juntá-las\n",
      "para responder à pergunta mais ampla.\n",
      "\n",
      "Aqui está uma pergunta: O que é uma equação quadrática?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'O que é uma equação quadrática?',\n",
       " 'text': 'Uma equação quadrática é uma equação polinomial de segunda ordem, ou seja, uma equação na qual a incógnita é elevada ao quadrado. Geralmente é escrita na forma ax^2 + bx + c = 0, onde a, b e c são constantes e x é a incógnita. A solução de uma equação quadrática pode ser encontrada utilizando a fórmula de Bhaskara ou completando o quadrado, dependendo do caso. As soluções de uma equação quadrática podem ser reais ou complexas.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'input': 'O que é uma equação quadrática?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMRouterChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "História: {'input': 'Quando ocorreu a Revolução Industrial?'}\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: Você é um historiador muito bom.\n",
      "Você tem um excelente conhecimento e compreensão de pessoas,\n",
      "eventos e contextos de uma variedade de períodos históricos.\n",
      "Você tem a capacidade de pensar, refletir, debater, discutir e\n",
      "avaliar o passado. Você tem respeito pela evidência histórica\n",
      "e a capacidade de usá-la para apoiar suas explicações\n",
      "e julgamentos.\n",
      "\n",
      "Aqui está uma pergunta: Quando ocorreu a Revolução Industrial?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Quando ocorreu a Revolução Industrial?',\n",
       " 'text': 'A Revolução Industrial ocorreu principalmente durante o século XVIII e XIX, com início na Inglaterra por volta do final do século XVIII. Este período foi marcado por avanços significativos na tecnologia, produção industrial, urbanização e mudanças sociais, que tiveram um impacto profundo na economia, política e sociedade em todo o mundo.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'input': 'Quando ocorreu a Revolução Industrial?'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
