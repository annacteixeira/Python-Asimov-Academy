{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelos de Linguagem Graandes (LLMs, na sigla em inglês) são um componente central do LangChain. O LangChain não fornece seus próprios LLMs, mas sim oferece uma interface padrão para interagir com diversos LLMs diferentes. Para ser específico, essa interface é uma que recebe como entrada uma string e retorna uma string.\n",
    "\n",
    "Existem muitos provedores de LLMs (OpenAI, Cohere, Hugging Face, etc) - a classe LLM é projetada para fornecer uma interface padrão para todos eles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annaf\\AppData\\Local\\Temp\\ipykernel_10328\\1267886439.py:6: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  llm = OpenAI(model='gpt-3.5-turbo-instruct')\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model='gpt-3.5-turbo-instruct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chamando a LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Samantha had always been fascinated by technology and computers. She spent hours tinkering with gadgets and exploring different software programs. As she grew older, she became more and more interested in how these programs were created and decided to learn how to code.\n",
      "\n",
      "At first, Samantha was overwhelmed by the seemingly complex world of programming. She didn't know where to start or what language to learn. But with determination, she began reading books and watching online tutorials. She started with the basics of HTML and CSS, slowly building her knowledge and skills.\n",
      "\n",
      "As she continued to learn, Samantha faced challenges and encountered errors in her code. But she didn't give up. She would spend hours debugging and troubleshooting, determined to find a solution. Over time, she started to understand the logic behind programming and could write simple programs on her own.\n",
      "\n",
      "Samantha's hard work paid off when she landed an internship at a software company. There, she had the opportunity to work with experienced programmers and learn from them. She was able to apply her knowledge and skills to real-world projects and see the impact of her code.\n",
      "\n",
      "Through this journey, Samantha realized that programming was not just about writing lines of code, but it was a way of thinking and problem-solving. She was proud of how far she"
     ]
    }
   ],
   "source": [
    "question = 'Tell a brief story about learning how to program'\n",
    "for text in llm.stream(question):\n",
    "    print(text, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chamadas simultâneas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\n\\nSky is the atmosphere or outer space seen from the Earth. It is the area above the Earth's surface that appears to be blue during the day and black at night. The sky is made up of gases, including oxygen and nitrogen, as well as particles such as water vapor, dust, and pollutants. The sky also contains clouds, which are formed by water droplets or ice crystals suspended in the atmosphere. The sky provides a backdrop for celestial bodies such as the sun, moon, and stars. \",\n",
       " \"\\n\\nEarth is the third planet from the Sun and the only known planet to support life. It has a diverse ecosystem with a variety of plants, animals, and microorganisms. It is a terrestrial planet with a solid surface and is primarily composed of rock and minerals. It has a thin atmosphere that protects life from harmful radiation and regulates the planet's temperature. Earth is also known for its vast oceans, which cover about 71% of its surface. It rotates on its axis, causing day and night, and orbits around the Sun, completing one revolution every 365.24 days. Earth is the home to millions of species, including humans, and is a unique and precious planet in our solar system.\",\n",
       " '\\n\\nStars are massive, luminous spheres of plasma held together by their own gravity. They emit light and heat through nuclear fusion reactions in their cores, which convert hydrogen into helium and release enormous amounts of energy. Stars vary in size, color, and temperature, and can be classified based on these characteristics. They play a crucial role in the formation and evolution of galaxies, and are responsible for the creation of elements necessary for life.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = [\n",
    "    'What is sky?',\n",
    "    'What is Earth?',\n",
    "    'What are stars?'\n",
    "]\n",
    "\n",
    "llm.batch(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatModels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatModels são um componente central do LangChain. \n",
    "\n",
    "Um modelo de chat é um modelo de linguagem que utiliza mensagens de chat como entradas e retorna mensagens de chat como saídas (ao invés de usar texto puro).\n",
    "\n",
    "O LangChain possui integrações com vários provedores de modelos (OpenAI, Cohere, Hugging Face, etc.) e expõe uma interface padrão para interagir com todos esses modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(model='gpt-3.5-turbo-0125')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content='You are an assistant that tells jokes'),\n",
    "    HumanMessage(content='Quanto é 1 + 1?')\n",
    "]\n",
    "\n",
    "answer = chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depende, se for em um mundo matemático, é 2. Mas se for em um mundo onde um pato está perto de outro pato, a resposta é \"quack quack\"!\n"
     ]
    }
   ],
   "source": [
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 45,\n",
       "  'prompt_tokens': 27,\n",
       "  'total_tokens': 72,\n",
       "  'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "   'audio_tokens': 0,\n",
       "   'reasoning_tokens': 0,\n",
       "   'rejected_prediction_tokens': 0},\n",
       "  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
       " 'model_name': 'gpt-3.5-turbo-0125',\n",
       " 'system_fingerprint': None,\n",
       " 'finish_reason': 'stop',\n",
       " 'logprobs': None}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer.response_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depende, estamos falando de matemática ou de um jogo de adivinhação? Porque se for matemática, a resposta é 2. Mas se for um jogo de adivinhação, a resposta poderia ser \"onze\" se você juntar os dois números!"
     ]
    }
   ],
   "source": [
    "# Usando streams\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content='You are an assistant that tells jokes'),\n",
    "    HumanMessage(content='Quanto é 1 + 1?')\n",
    "]\n",
    "\n",
    "for text in chat.stream(messages):\n",
    "    print(text.content, end = '')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem 5 tipos de mensagens diferentes:\n",
    " - HumanMessage: representa uma mensagem do usuário. Geralmente consiste apenas de conteúdo.\n",
    " - AIMessage: representa uma mensagem do modelo. Pode ter additional_kwargs incluídos - por exemplo, tool_calls se estiver usando chamadas de ferramentas da OpenAI.\n",
    " - SystemMessage: representa uma mensagem do sistema, que indica ao modelo como se comportar. Geralmente consiste apenas de conteúdo. Nem todo modelo suporta isso.\n",
    " - FunctionMessage: representa o resultado de uma chamada de função. Além do papel e conteúdo, esta mensagem tem um parâmetro de nome que transmite o nome da função que foi chamada para produzir este resultado.\n",
    " - ToolMessage: representa o resultado de uma chamada de ferramenta. Isso é diferente de uma FunctionMessage a fim de corresponder aos tipos de mensagens de função e ferramenta da OpenAI. Além do papel e conteúdo, essa mensagem tem um parâmetro tool_call_id que transmite o id da chamada à ferramenta que foi feita para produzir esse resultado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Few-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='13', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 2, 'prompt_tokens': 52, 'total_tokens': 54, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-02421162-b6ed-4c41-894a-50044c094279-0', usage_metadata={'input_tokens': 52, 'output_tokens': 2, 'total_tokens': 54, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(content='How much is 1 + 1?'),\n",
    "    AIMessage(content='2'),\n",
    "    HumanMessage(content='How much is 10 * 5?'),\n",
    "    AIMessage(content='50'),\n",
    "    HumanMessage(content='How much is 10 + 3?'),\n",
    "]\n",
    "\n",
    "chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizando outros modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models.huggingface import ChatHuggingFace\n",
    "from langchain_community.llms.huggingface_endpoint import HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'mistralai/Mixtral-8x7B-Instruct-v0.1'\n",
    "\n",
    "llm = HuggingFaceEndpoint(repo_id=model)\n",
    "chat = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=' 13. The sum of 10 and 3 is 13.', additional_kwargs={}, response_metadata={}, id='run-f3ccd200-a853-49a2-9b2f-ced044b8c48f-0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(content='How much is 1 + 1?'),\n",
    "    AIMessage(content='2'),\n",
    "    HumanMessage(content='How much is 10 * 5?'),\n",
    "    AIMessage(content='50'),\n",
    "    HumanMessage(content='How much is 10 + 3?'),\n",
    "]\n",
    "\n",
    "chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A estrutura de chat_model utiliza a estrutura de LLM como backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You are an assistant that tells jokes\\nHuman: Quanto é 1 + 1?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[llm:ChatOpenAI] [0ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Depende, em que base você está contando? Em base 2, 1 + 1 = 10!\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Depende, em que base você está contando? Em base 2, 1 + 1 = 10!\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 26,\n",
      "                \"prompt_tokens\": 27,\n",
      "                \"total_tokens\": 53,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"accepted_prediction_tokens\": 0,\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"reasoning_tokens\": 0,\n",
      "                  \"rejected_prediction_tokens\": 0\n",
      "                },\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"cached_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-d057a4f7-4cb2-4041-8494-da3200a57a53-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 27,\n",
      "              \"output_tokens\": 26,\n",
      "              \"total_tokens\": 53,\n",
      "              \"input_token_details\": {\n",
      "                \"audio\": 0,\n",
      "                \"cache_read\": 0\n",
      "              },\n",
      "              \"output_token_details\": {\n",
      "                \"audio\": 0,\n",
      "                \"reasoning\": 0\n",
      "              }\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "\n",
    "langchain.debug = True\n",
    "chat.invoke(messages)\n",
    "langchain.debug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache em memória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(model='gpt-3.5-turbo-0125')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content='You are an assistant that tells jokes'),\n",
    "    HumanMessage(content='Quanto é 1 + 1?')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import InMemoryCache\n",
    "from langchain.globals import set_llm_cache\n",
    "\n",
    "set_llm_cache(InMemoryCache())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executando pela primeira vez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Depende, em que base você está contando? Em base 2, 1 + 1 = 10!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 27, 'total_tokens': 53, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d057a4f7-4cb2-4041-8494-da3200a57a53-0', usage_metadata={'input_tokens': 27, 'output_tokens': 26, 'total_tokens': 53, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import SQLiteCache\n",
    "from langchain.globals import set_llm_cache\n",
    "\n",
    "set_llm_cache(SQLiteCache(database_path='files/langchain_cache_db.sqlite'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 685 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Depende, estás a falar de matemática ou de amor? Porque, se for matemática, é 2. Mas se for amor, é 11!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 27, 'total_tokens': 66, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-faff50d0-3c1d-4b51-b231-d8f149fa5778-0', usage_metadata={'input_tokens': 27, 'output_tokens': 39, 'total_tokens': 66, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executando novamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 93.8 ms\n",
      "Wall time: 87.4 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Depende, estás a falar de matemática ou de amor? Porque, se for matemática, é 2. Mas se for amor, é 11!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 27, 'total_tokens': 66, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-faff50d0-3c1d-4b51-b231-d8f149fa5778-0', usage_metadata={'input_tokens': 27, 'output_tokens': 39, 'total_tokens': 66, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "chat.invoke(messages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
