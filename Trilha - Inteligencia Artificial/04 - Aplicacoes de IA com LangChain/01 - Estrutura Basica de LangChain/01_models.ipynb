{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelos de Linguagem Graandes (LLMs, na sigla em inglês) são um componente central do LangChain. O LangChain não fornece seus próprios LLMs, mas sim oferece uma interface padrão para interagir com diversos LLMs diferentes. Para ser específico, essa interface é uma que recebe como entrada uma string e retorna uma string.\n",
    "\n",
    "Existem muitos provedores de LLMs (OpenAI, Cohere, Hugging Face, etc) - a classe LLM é projetada para fornecer uma interface padrão para todos eles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annaf\\AppData\\Local\\Temp\\ipykernel_10328\\1267886439.py:6: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  llm = OpenAI(model='gpt-3.5-turbo-instruct')\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model='gpt-3.5-turbo-instruct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chamando a LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Samantha had always been fascinated by technology and computers. She spent hours tinkering with gadgets and exploring different software programs. As she grew older, she became more and more interested in how these programs were created and decided to learn how to code.\n",
      "\n",
      "At first, Samantha was overwhelmed by the seemingly complex world of programming. She didn't know where to start or what language to learn. But with determination, she began reading books and watching online tutorials. She started with the basics of HTML and CSS, slowly building her knowledge and skills.\n",
      "\n",
      "As she continued to learn, Samantha faced challenges and encountered errors in her code. But she didn't give up. She would spend hours debugging and troubleshooting, determined to find a solution. Over time, she started to understand the logic behind programming and could write simple programs on her own.\n",
      "\n",
      "Samantha's hard work paid off when she landed an internship at a software company. There, she had the opportunity to work with experienced programmers and learn from them. She was able to apply her knowledge and skills to real-world projects and see the impact of her code.\n",
      "\n",
      "Through this journey, Samantha realized that programming was not just about writing lines of code, but it was a way of thinking and problem-solving. She was proud of how far she"
     ]
    }
   ],
   "source": [
    "question = 'Tell a brief story about learning how to program'\n",
    "for text in llm.stream(question):\n",
    "    print(text, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chamadas simultâneas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\n\\nSky is the atmosphere or outer space seen from the Earth. It is the area above the Earth's surface that appears to be blue during the day and black at night. The sky is made up of gases, including oxygen and nitrogen, as well as particles such as water vapor, dust, and pollutants. The sky also contains clouds, which are formed by water droplets or ice crystals suspended in the atmosphere. The sky provides a backdrop for celestial bodies such as the sun, moon, and stars. \",\n",
       " \"\\n\\nEarth is the third planet from the Sun and the only known planet to support life. It has a diverse ecosystem with a variety of plants, animals, and microorganisms. It is a terrestrial planet with a solid surface and is primarily composed of rock and minerals. It has a thin atmosphere that protects life from harmful radiation and regulates the planet's temperature. Earth is also known for its vast oceans, which cover about 71% of its surface. It rotates on its axis, causing day and night, and orbits around the Sun, completing one revolution every 365.24 days. Earth is the home to millions of species, including humans, and is a unique and precious planet in our solar system.\",\n",
       " '\\n\\nStars are massive, luminous spheres of plasma held together by their own gravity. They emit light and heat through nuclear fusion reactions in their cores, which convert hydrogen into helium and release enormous amounts of energy. Stars vary in size, color, and temperature, and can be classified based on these characteristics. They play a crucial role in the formation and evolution of galaxies, and are responsible for the creation of elements necessary for life.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = [\n",
    "    'What is sky?',\n",
    "    'What is Earth?',\n",
    "    'What are stars?'\n",
    "]\n",
    "\n",
    "llm.batch(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatModels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatModels são um componente central do LangChain. \n",
    "\n",
    "Um modelo de chat é um modelo de linguagem que utiliza mensagens de chat como entradas e retorna mensagens de chat como saídas (ao invés de usar texto puro).\n",
    "\n",
    "O LangChain possui integrações com vários provedores de modelos (OpenAI, Cohere, Hugging Face, etc.) e expõe uma interface padrão para interagir com todos esses modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(model='gpt-3.5-turbo-0125')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content='You are an assistant that tells jokes'),\n",
    "    HumanMessage(content='Quanto é 1 + 1?')\n",
    "]\n",
    "\n",
    "answer = chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depende, se for em um mundo matemático, é 2. Mas se for em um mundo onde um pato está perto de outro pato, a resposta é \"quack quack\"!\n"
     ]
    }
   ],
   "source": [
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 45,\n",
       "  'prompt_tokens': 27,\n",
       "  'total_tokens': 72,\n",
       "  'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "   'audio_tokens': 0,\n",
       "   'reasoning_tokens': 0,\n",
       "   'rejected_prediction_tokens': 0},\n",
       "  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
       " 'model_name': 'gpt-3.5-turbo-0125',\n",
       " 'system_fingerprint': None,\n",
       " 'finish_reason': 'stop',\n",
       " 'logprobs': None}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer.response_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depende, estamos falando de matemática ou de um jogo de adivinhação? Porque se for matemática, a resposta é 2. Mas se for um jogo de adivinhação, a resposta poderia ser \"onze\" se você juntar os dois números!"
     ]
    }
   ],
   "source": [
    "# Usando streams\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content='You are an assistant that tells jokes'),\n",
    "    HumanMessage(content='Quanto é 1 + 1?')\n",
    "]\n",
    "\n",
    "for text in chat.stream(messages):\n",
    "    print(text.content, end = '')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem 5 tipos de mensagens diferentes:\n",
    " - HumanMessage: representa uma mensagem do usuário. Geralmente consiste apenas de conteúdo.\n",
    " - AIMessage: representa uma mensagem do modelo. Pode ter additional_kwargs incluídos - por exemplo, tool_calls se estiver usando chamadas de ferramentas da OpenAI.\n",
    " - SystemMessage: representa uma mensagem do sistema, que indica ao modelo como se comportar. Geralmente consiste apenas de conteúdo. Nem todo modelo suporta isso.\n",
    " - FunctionMessage: representa o resultado de uma chamada de função. Além do papel e conteúdo, esta mensagem tem um parâmetro de nome que transmite o nome da função que foi chamada para produzir este resultado.\n",
    " - ToolMessage: representa o resultado de uma chamada de ferramenta. Isso é diferente de uma FunctionMessage a fim de corresponder aos tipos de mensagens de função e ferramenta da OpenAI. Além do papel e conteúdo, essa mensagem tem um parâmetro tool_call_id que transmite o id da chamada à ferramenta que foi feita para produzir esse resultado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Few-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='13', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 2, 'prompt_tokens': 52, 'total_tokens': 54, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-02421162-b6ed-4c41-894a-50044c094279-0', usage_metadata={'input_tokens': 52, 'output_tokens': 2, 'total_tokens': 54, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(content='How much is 1 + 1?'),\n",
    "    AIMessage(content='2'),\n",
    "    HumanMessage(content='How much is 10 * 5?'),\n",
    "    AIMessage(content='50'),\n",
    "    HumanMessage(content='How much is 10 + 3?'),\n",
    "]\n",
    "\n",
    "chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizando outros modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models.huggingface import ChatHuggingFace\n",
    "from langchain_community.llms.huggingface_endpoint import HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'mistralai/Mixtral-8x7B-Instruct-v0.1'\n",
    "\n",
    "llm = HuggingFaceEndpoint(repo_id=model)\n",
    "chat = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=' 13. The sum of 10 and 3 is 13.', additional_kwargs={}, response_metadata={}, id='run-f3ccd200-a853-49a2-9b2f-ced044b8c48f-0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(content='How much is 1 + 1?'),\n",
    "    AIMessage(content='2'),\n",
    "    HumanMessage(content='How much is 10 * 5?'),\n",
    "    AIMessage(content='50'),\n",
    "    HumanMessage(content='How much is 10 + 3?'),\n",
    "]\n",
    "\n",
    "chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A estrutura de chat_model utiliza a estrutura de LLM como backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You are an assistant that tells jokes\\nHuman: Quanto é 1 + 1?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[llm:ChatOpenAI] [0ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Depende, em que base você está contando? Em base 2, 1 + 1 = 10!\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Depende, em que base você está contando? Em base 2, 1 + 1 = 10!\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 26,\n",
      "                \"prompt_tokens\": 27,\n",
      "                \"total_tokens\": 53,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"accepted_prediction_tokens\": 0,\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"reasoning_tokens\": 0,\n",
      "                  \"rejected_prediction_tokens\": 0\n",
      "                },\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"cached_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-d057a4f7-4cb2-4041-8494-da3200a57a53-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 27,\n",
      "              \"output_tokens\": 26,\n",
      "              \"total_tokens\": 53,\n",
      "              \"input_token_details\": {\n",
      "                \"audio\": 0,\n",
      "                \"cache_read\": 0\n",
      "              },\n",
      "              \"output_token_details\": {\n",
      "                \"audio\": 0,\n",
      "                \"reasoning\": 0\n",
      "              }\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "\n",
    "langchain.debug = True\n",
    "chat.invoke(messages)\n",
    "langchain.debug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache em memória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(model='gpt-3.5-turbo-0125')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content='You are an assistant that tells jokes'),\n",
    "    HumanMessage(content='Quanto é 1 + 1?')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import InMemoryCache\n",
    "from langchain.globals import set_llm_cache\n",
    "\n",
    "set_llm_cache(InMemoryCache())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executando pela primeira vez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Depende, em que base você está contando? Em base 2, 1 + 1 = 10!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 27, 'total_tokens': 53, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d057a4f7-4cb2-4041-8494-da3200a57a53-0', usage_metadata={'input_tokens': 27, 'output_tokens': 26, 'total_tokens': 53, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import SQLiteCache\n",
    "from langchain.globals import set_llm_cache\n",
    "\n",
    "set_llm_cache(SQLiteCache(database_path='files/langchain_cache_db.sqlite'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 685 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Depende, estás a falar de matemática ou de amor? Porque, se for matemática, é 2. Mas se for amor, é 11!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 27, 'total_tokens': 66, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-faff50d0-3c1d-4b51-b231-d8f149fa5778-0', usage_metadata={'input_tokens': 27, 'output_tokens': 39, 'total_tokens': 66, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executando novamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 93.8 ms\n",
      "Wall time: 87.4 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Depende, estás a falar de matemática ou de amor? Porque, se for matemática, é 2. Mas se for amor, é 11!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 27, 'total_tokens': 66, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-faff50d0-3c1d-4b51-b231-d8f149fa5778-0', usage_metadata={'input_tokens': 27, 'output_tokens': 39, 'total_tokens': 66, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um prompt para um modelo de linguagem é um conjunto de instruções ou entradas fornecidas por um usuário para guiar a resposta do modelo, ajudando-o a entender o contexto e gerar uma saída baseada em linguagem relevante e coerente, como responder a perguntas, completar frases ou participar de uma conversa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.llms import OpenAI\n",
    "\n",
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template('''\n",
    "    Answer the following question from the user using up to {n_words} words:\n",
    "    {question}\n",
    "''', partial_variables={'n_words': 50})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Answer the following question from the user using up to 100 words:\n",
      "    What is a black hole?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt_template.format(n_words= 100, question='What is a black hole?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composing prompts | Unindo múltiplos prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['language', 'n_words', 'question'], input_types={}, partial_variables={}, template='\\n    Answer the question using up to {n_words} words.                                                \\n\\n    Return the answer in {language} language.                                                  \\nAnswer the question following the instructions: {question}')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai.llms import OpenAI\n",
    "\n",
    "template_word_count = PromptTemplate.from_template('''\n",
    "    Answer the question using up to {n_words} words.                                                \n",
    "''')\n",
    "\n",
    "template_language = PromptTemplate.from_template('''\n",
    "    Return the answer in {language} language.                                                  \n",
    "''')\n",
    "\n",
    "final_template = template_word_count + template_language + 'Answer the question following the instructions: {question}'\n",
    "\n",
    "final_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nA star is a luminous sphere of plasma held together by its own gravity.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = final_template.format(n_words=10, language='english', question='What is a star?')\n",
    "\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Answer the question using up to 10 words.                                                \n",
      "\n",
      "    Return the answer in english language.                                                  \n",
      "Answer the question following the instructions: What is a star?\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Templates para Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='This is my question: Who am I?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_template('This is my question: {question}')\n",
    "\n",
    "chat_template.format_messages(question='Who am I?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['assistant_name', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['assistant_name'], input_types={}, partial_variables={}, template='You are a funny assistant and your name is {assistant_name}'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Hi, how are you?'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Better now! How can I help you?'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a funny assistant and your name is {assistant_name}'),\n",
    "    ('human', 'Hi, how are you?'),\n",
    "    ('ai', 'Better now! How can I help you?'),\n",
    "    ('human', '{question}')\n",
    "])\n",
    "\n",
    "chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a funny assistant and your name is Asimov', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hi, how are you?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Better now! How can I help you?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_template.format_messages(assistant_name='Asimov', question='What is your name?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='My name is Asimov, your friendly neighborhood assistant! How can I make you smile today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 51, 'total_tokens': 72, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-74310a13-5435-446c-ab54-f947899bea82-0', usage_metadata={'input_tokens': 51, 'output_tokens': 21, 'total_tokens': 72, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "chat.invoke(chat_template.format_messages(assistant_name='Asimov', question='What is your name?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"The meaning of life is a question that has puzzled humans for centuries. Some say it's to seek happiness, others say it's to find fulfillment or purpose. As for me, I'm just here to provide assistance and a little bit of humor along the way!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 53, 'total_tokens': 107, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-da10047a-dd58-4f26-8640-acf4c4d0dc96-0', usage_metadata={'input_tokens': 53, 'output_tokens': 54, 'total_tokens': 107, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(chat_template.format_messages(assistant_name='Asimov', question='What is the meaning of life?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Templates de Few-Shot Prompting para LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "exemplos = [\n",
    "    {\"pergunta\": \"Quem viveu mais tempo, Muhammad Ali ou Alan Turing?\", \n",
    "     \"resposta\": \n",
    "     \"\"\"São necessárias perguntas de acompanhamento aqui: Sim. \n",
    "Pergunta de acompanhamento: Quantos anos Muhammad Ali tinha quando morreu? \n",
    "Resposta intermediária: Muhammad Ali tinha 74 anos quando morreu. \n",
    "Pergunta de acompanhamento: Quantos anos Alan Turing tinha quando morreu? \n",
    "Resposta intermediária: Alan Turing tinha 41 anos quando morreu. \n",
    "Então a resposta final é: Muhammad Ali \n",
    "\"\"\", \n",
    "    }, \n",
    "    {\"pergunta\": \"Quando nasceu o fundador do craigslist?\", \n",
    "     \"resposta\": \n",
    "\"\"\"São necessárias perguntas de acompanhamento aqui: Sim. \n",
    "Pergunta de acompanhamento: Quem foi o fundador do craigslist? \n",
    "Resposta intermediária: O craigslist foi fundado por Craig Newmark. \n",
    "Pergunta de acompanhamento: Quando nasceu Craig Newmark? \n",
    "Resposta intermediária: Craig Newmark nasceu em 6 de dezembro de 1952. \n",
    "Então a resposta final é: 6 de dezembro de 1952 \n",
    "\"\"\", \n",
    "    }, \n",
    "    {\"pergunta\": \"Quem foi o avô materno de George Washington?\",\n",
    "     \"resposta\": \n",
    "\"\"\"São necessárias perguntas de acompanhamento aqui: Sim. \n",
    "Pergunta de acompanhamento: Quem foi a mãe de George Washington? \n",
    "Resposta intermediária: A mãe de George Washington foi Mary Ball Washington. \n",
    "Pergunta de acompanhamento: Quem foi o pai de Mary Ball Washington? \n",
    "Resposta intermediária: O pai de Mary Ball Washington foi Joseph Ball. \n",
    "Então a resposta final é: Joseph Ball \n",
    "\"\"\", \n",
    "    },\n",
    "    {\"pergunta\": \"Os diretores de Jaws e Casino Royale são do mesmo país?\", \n",
    "     \"resposta\": \n",
    "\"\"\"São necessárias perguntas de acompanhamento aqui: Sim. \n",
    "Pergunta de acompanhamento: Quem é o diretor de Jaws? \n",
    "Resposta Intermediária: O diretor de Jaws é Steven Spielberg. \n",
    "Pergunta de acompanhamento: De onde é Steven Spielberg? \n",
    "Resposta Intermediária: Estados Unidos. \n",
    "Pergunta de acompanhamento: Quem é o diretor de Casino Royale? \n",
    "Resposta Intermediária: O diretor de Casino Royale é Martin Campbell. \n",
    "Pergunta de acompanhamento: De onde é Martin Campbell? \n",
    "Resposta Intermediária: Nova Zelândia. \n",
    "Então a resposta final é: Não \n",
    "\"\"\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pergunta Quem viveu mais tempo, Muhammad Ali ou Alan Turing?\\nSão necessárias perguntas de acompanhamento aqui: Sim. \\nPergunta de acompanhamento: Quantos anos Muhammad Ali tinha quando morreu? \\nResposta intermediária: Muhammad Ali tinha 74 anos quando morreu. \\nPergunta de acompanhamento: Quantos anos Alan Turing tinha quando morreu? \\nResposta intermediária: Alan Turing tinha 41 anos quando morreu. \\nEntão a resposta final é: Muhammad Ali \\n'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=['pergunta', 'resposta'],\n",
    "    template='Pergunta {pergunta}\\n{resposta}'\n",
    ")\n",
    "\n",
    "example_prompt.format(**exemplos[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = FewShotPromptTemplate(\n",
    "    examples = exemplos,\n",
    "    example_prompt = example_prompt,\n",
    "    suffix = 'Pergunta: {input}',\n",
    "    input_variables = ['input']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pergunta Quem viveu mais tempo, Muhammad Ali ou Alan Turing?\n",
      "São necessárias perguntas de acompanhamento aqui: Sim. \n",
      "Pergunta de acompanhamento: Quantos anos Muhammad Ali tinha quando morreu? \n",
      "Resposta intermediária: Muhammad Ali tinha 74 anos quando morreu. \n",
      "Pergunta de acompanhamento: Quantos anos Alan Turing tinha quando morreu? \n",
      "Resposta intermediária: Alan Turing tinha 41 anos quando morreu. \n",
      "Então a resposta final é: Muhammad Ali \n",
      "\n",
      "\n",
      "Pergunta Quando nasceu o fundador do craigslist?\n",
      "São necessárias perguntas de acompanhamento aqui: Sim. \n",
      "Pergunta de acompanhamento: Quem foi o fundador do craigslist? \n",
      "Resposta intermediária: O craigslist foi fundado por Craig Newmark. \n",
      "Pergunta de acompanhamento: Quando nasceu Craig Newmark? \n",
      "Resposta intermediária: Craig Newmark nasceu em 6 de dezembro de 1952. \n",
      "Então a resposta final é: 6 de dezembro de 1952 \n",
      "\n",
      "\n",
      "Pergunta Quem foi o avô materno de George Washington?\n",
      "São necessárias perguntas de acompanhamento aqui: Sim. \n",
      "Pergunta de acompanhamento: Quem foi a mãe de George Washington? \n",
      "Resposta intermediária: A mãe de George Washington foi Mary Ball Washington. \n",
      "Pergunta de acompanhamento: Quem foi o pai de Mary Ball Washington? \n",
      "Resposta intermediária: O pai de Mary Ball Washington foi Joseph Ball. \n",
      "Então a resposta final é: Joseph Ball \n",
      "\n",
      "\n",
      "Pergunta Os diretores de Jaws e Casino Royale são do mesmo país?\n",
      "São necessárias perguntas de acompanhamento aqui: Sim. \n",
      "Pergunta de acompanhamento: Quem é o diretor de Jaws? \n",
      "Resposta Intermediária: O diretor de Jaws é Steven Spielberg. \n",
      "Pergunta de acompanhamento: De onde é Steven Spielberg? \n",
      "Resposta Intermediária: Estados Unidos. \n",
      "Pergunta de acompanhamento: Quem é o diretor de Casino Royale? \n",
      "Resposta Intermediária: O diretor de Casino Royale é Martin Campbell. \n",
      "Pergunta de acompanhamento: De onde é Martin Campbell? \n",
      "Resposta Intermediária: Nova Zelândia. \n",
      "Então a resposta final é: Não \n",
      "\n",
      "\n",
      "Pergunta: Quem fez mais gols, Romário ou Pelé?\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(input='Quem fez mais gols, Romário ou Pelé?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nSão necessárias perguntas de acompanhamento aqui: Não. \\nEntão a resposta final é: Pelé'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(prompt.format(input='Quem fez mais gols, Romário ou Pelé?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template de Few-Shot Prompting para Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='Quem viveu mais tempo, Muhammad Ali ou Alan Turing?', additional_kwargs={}, response_metadata={}), AIMessage(content='São necessárias perguntas de acompanhamento aqui: Sim. \\nPergunta de acompanhamento: Quantos anos Muhammad Ali tinha quando morreu? \\nResposta intermediária: Muhammad Ali tinha 74 anos quando morreu. \\nPergunta de acompanhamento: Quantos anos Alan Turing tinha quando morreu? \\nResposta intermediária: Alan Turing tinha 41 anos quando morreu. \\nEntão a resposta final é: Muhammad Ali \\n', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [('human', '{pergunta}'),\n",
    "     ('ai', '{resposta}')]\n",
    ")\n",
    "\n",
    "print(example_prompt.format_messages(**exemplos[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_show_template = FewShotChatMessagePromptTemplate(\n",
    "    examples=exemplos,\n",
    "    example_prompt=example_prompt\n",
    ")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [few_show_template,\n",
    "    ('human', '{input}')]\n",
    ")\n",
    "\n",
    "prompt = prompt_template.format_messages(input='Quem fez mais gols, Romário ou Pelé?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Pelé marcou mais gols ao longo de sua carreira do que Romário. Pelé é considerado um dos maiores artilheiros da história do futebol, com mais de 1.000 gols marcados em jogos oficiais e amistosos. Romário, por sua vez, também teve uma carreira de sucesso e marcou muitos gols, mas não alcançou a marca impressionante de Pelé.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 98, 'prompt_tokens': 538, 'total_tokens': 636, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-bd224c08-619a-421c-8643-64e7ba044ca9-0', usage_metadata={'input_tokens': 538, 'output_tokens': 98, 'total_tokens': 636, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os analisadores de saída são responsáveis por pegar a saída de um LLM e transformá-la em um formato mais adequado. Isso é muito útil quando você está usando LLMs para gerar qualquer forma de dados estruturados.\n",
    "\n",
    "Digamos que temos a seguinte review de um produto:\n",
    "\n",
    "> \"Este soprador de folhas é incrível. Ele tem quatro configurações: sopro de vela, brisa suave, cidade ventosa e tornado. Chegou em dois dias, bem a tempo para o presente de aniversário da minha esposa. Acho que minha esposa gostou tanto que ficou sem palavras. Até agora, fui o único a usá-lo, e tenho usado em todas as manhãs alternadas para limpar as folhas do nosso gramado. É um pouco mais caro do que os outros sopradores de folhas disponíveis no mercado, mas acho que vale a pena pelas características extras.\"\n",
    "\n",
    "E eu quero que o modelo de linguagem processe esta review para estruturá-la no seguinte formato:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"presente\": true,\n",
    "    \"dias_entrega\": 2,\n",
    "    \"percepcao_de_valor\": [\"um pouco mais caro do que os outros sopradores de folhas disponíveis no mercado\"]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"presente\": bool  // O item foi comprado para alguém? True caso verdadeiro e False caso falso ou não tenha a informação\n",
      "\t\"dias_entrega\": int  // Quantos dias para a entrega chegar? Se a resposta não for encontrada, retorne -1\n",
      "\t\"percepcao_de_valor\": list  // Extraia qualquer frase sobre o valor ou preço do produto. Retorne como uma lista de Python\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "schema_presente = ResponseSchema(\n",
    "    name='presente',\n",
    "    type='bool',\n",
    "    description='O item foi comprado para alguém? True caso verdadeiro e False caso falso ou não tenha a informação'\n",
    ")\n",
    "\n",
    "schema_entrega = ResponseSchema(\n",
    "    name='dias_entrega',\n",
    "    type='int',\n",
    "    description='Quantos dias para a entrega chegar? Se a resposta não for encontrada, retorne -1'\n",
    ")\n",
    "\n",
    "schema_valor = ResponseSchema(\n",
    "    name='percepcao_de_valor',\n",
    "    type='list',\n",
    "    description='Extraia qualquer frase sobre o valor ou preço do produto. Retorne como uma lista de Python'\n",
    ")\n",
    "\n",
    "response_schema = [schema_presente, schema_entrega, schema_valor]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schema)\n",
    "\n",
    "schema_formatado = output_parser.get_format_instructions()\n",
    "print(schema_formatado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "review_cliente = '''\n",
    "    Este soprador de folhas é incrível. Ele tem quatro configurações: sopro de vela, brisa suave, cidade ventosa e tornado. Chegou em dois dias, bem a tempo para o presente de aniversário da minha esposa. Acho que minha esposa gostou tanto que ficou sem palavras. Até agora, fui o único a usá-lo, e tenho usado em todas as manhãs alternadas para limpar as folhas do nosso gramado. É um pouco mais caro do que os outros sopradores de folhas disponíveis no mercado, mas acho que vale a pena pelas características extras.\n",
    "'''\n",
    "\n",
    "review_template = ChatPromptTemplate.from_template('''\n",
    "    Para o texto a seguir, extraia as seguintes informações:\n",
    "    \n",
    "    presente, dias_entrega e percepcao_de_valor\n",
    "    \n",
    "    Texto: {review}           \n",
    "    \n",
    "    {schema}                        \n",
    "''', partial_variables={'schema': schema_formatado})\n",
    "\n",
    "analise = review_template.format_messages(review=review_cliente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"presente\": true,\n",
      "\t\"dias_entrega\": 2,\n",
      "\t\"percepcao_de_valor\": [\"É um pouco mais caro do que os outros sopradores de folhas disponíveis no mercado, mas acho que vale a pena pelas características extras.\"]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "resposta = chat.invoke(analise)\n",
    "print(resposta.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "resposta_json = output_parser.parse(resposta.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resposta_json['presente']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resposta_json['dias_entrega']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['É um pouco mais caro do que os outros sopradores de folhas disponíveis no mercado, mas acho que vale a pena pelas características extras.']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resposta_json['percepcao_de_valor']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
